{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba8b9a34",
   "metadata": {},
   "source": [
    "# After class summary ... \n",
    "----\n",
    "Hope this somehow helps with revision .... \n",
    "\n",
    "\n",
    "\n",
    "# Bagging vs Boosting\n",
    "There were some confusion regarding the similarities and differences. \n",
    "## Similarities: \n",
    "Both falls under **ensemble method**. Meaning, both consist of \n",
    "   * a set number of **base learners** (which are generally presumed to be weak, i.e. low performance), $f_1, f_2, \\dots, f_K$. \n",
    "   * The base learners are themselves trained using some or all of the dataset. \n",
    "   * The trained base learners are combined in some way (usually straight majority vote or weight sum) to get the actual classifer. \n",
    "\n",
    "## Differences\n",
    "|             | Bagging | Boosting     |\n",
    "| :---        |    :----:   |          :---: |\n",
    "| Training | Can be done in parallel since each base learner is trained independently | Have to be sequential since each new base learner is trying to fix mistakes made by combination of previous base learners. |\n",
    "| Sampling      |  Can be done in parallel by doing sampling with replacement from the original data set    | Depends on previous generation and focus on samples previous generatation find \"difficult\".  |\n",
    "| Voting   | Simple majority vote        | Weighted sum      |\n",
    "| Effect on resulting classifier | Reduces variance (due to independent sampling) | Reduces bias (due to reclassifying wrongly classified difficult samples) |\n",
    "\n",
    "\n",
    "## Adaboost vs XGBoost\n",
    "Similarities: \n",
    " * Both are boosting algorithms. \n",
    " * Both can be done using arbitrary base learners. But they are frequently discussed / applied in the context where the base learners are decision trees. \n",
    " * Both aim to train a final classifier which is a sum of base classifiers $F(x) = \\sum_t f_t(x) = f_1(x) + f_2(x) + \\dots + f_T(x)$. (let's forget about the weights for now, later we can always do $f_t(x) = w_tf_t(x)$). \n",
    " * Both are trained iteratively with a new $f_t$ being trained based on what the **combination** of previous (already trained) learners, $F_{t-1} = f_1 + f_2 + \\dots + f_{t -1}$, thinks is difficult. \n",
    " * Both aim to minimise a training loss $L(F)$ (like most machine learning algorithms). And lets only focus on square loss $L(f) = \\sum_i (F(x_i) - y_i)^2$ and forget the regularisation term for a moment. \n",
    "\n",
    "\n",
    "Differences: \n",
    "| | Adaboost | XGBoost |\n",
    "| :--- | :---: | :---: |\n",
    "| Training data in each iteration | A **new** training set is sampled (with replacement) every iteration with a **changing** distribution $D_t$ that makes training samples incorrectly classified by previous iteration more likely to be sampled | Just use the whole dataset in every iteration | \n",
    "| Objective function in each iteration | Does not change. But do note that the objective function is taking average of a changing dataset every iteration | Changes every iteration since it includes the size of error made on each training sample in previous iteration. |\n",
    "\n",
    "\n",
    "**Details for Adaboost**\n",
    "TODO....\n",
    "\n",
    "**Details for XGBoost**\n",
    "TODO.... \n",
    "\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "# Some common themes in machine learning\n",
    "One goal of this section is to help you navigate the **terminology** you might encounter is machine learning. People give names to things because the name refer to something important but also common enough pattern that we don't need to come up with lots of names. Below we will see one illustrative common pattern. This is not the full scope of modern machine learning but it is illustrative of lots of common elements. \n",
    "\n",
    "## The task of learning\n",
    "There is usually a hidden function that we are trying to learn. \n",
    "\n",
    "$$\n",
    "y = f(x) \n",
    "$$\n",
    "\n",
    "and we try our best to come out with $\\hat{f}$ that does that. \n",
    "\n",
    "We might be doing regression, in which case, we want to answer: given predictor $x$, what could the response $y = f(x)$ be? \n",
    "\n",
    "Or we could be doing classification where we want $\\hat{f}(x)$ to output the correct class of the observation $x$. \n",
    "\n",
    "Or we could be trying to approximate the probability of `class = some class label` given observation $x$. \n",
    "\n",
    "\n",
    "\n",
    "Common element of machine learning: \n",
    "## Data \n",
    "We don't have access to the truth $f$, but we do have access to a list of data $D_N = \\{(x_1, y_1), \\dots, (x_N, y_N)\\}$ and we hope that the data is **representative** of the function, meaning it is not concentrated on one small region of $x$ for example. \n",
    "\n",
    "We have lots of names for $x$ - predictors, features, input, measurements...  \n",
    "And lots of names for $y$ - response, labels, output, value... \n",
    "\n",
    "$x$ can be a vector, $x = (x_j)_j = (x_1, x_2, \\dots, x_m)$, which means sometimes you might see actual numbers being labeled by two indices: \n",
    "\n",
    "$$\n",
    "x_i = (x_{ij})_j = (x_{i1}, x_{i2}, \\dots, x{im})\n",
    "$$\n",
    "where $x_{ij} =$ value of feature $j$ for data $i$. \n",
    "For example: \n",
    " * $y=$ whether or not it rains and $x$ is cloudiness. So we have one continuous features and one binary output. \n",
    " * $y=$ whether or not it rains and $x$ is cloudiness and whether it rained yesterday. We have one binary output, one continuous feature and one binary feature. \n",
    " * $y=$ probability of rain and $x$ is temperature and cloudiness. We have a continuous output and two dimensional continuous inputs. \n",
    "\n",
    "\n",
    "\n",
    "Furthermore, we have **noise** in our data, meaning we only have \n",
    "\n",
    "$$y_i = f(x_i) + \\epsilon_i$$ \n",
    "\n",
    "for each of the pair in $D_N$, not $y_i = f(x_i)$. This is one way statistics comes into learning. So not only do not know what $f$ is, we don't even know that the given $y_i$ in the dataset is exactly right. \n",
    "\n",
    "\n",
    "## Model\n",
    " This of this as family of possible guesses of $\\hat{f}$. And they are usually **parametrised**, so we write $\\hat{f}_\\theta$. \n",
    "\n",
    " The parameter $\\theta$ determines one particular guess of the estimate $f$. Parameters itself can live in a huge space for example $\\theta = (\\theta_1, \\dots, \\theta_d)$. \n",
    "\n",
    " Examples: \n",
    "  * Naive Bayes. The parameter space depends on the distribution you choose for the factors. For example, in Gaussian Naive Bayes, you still need to find the mean and variance for each $x_i$ and for each class $k$. \n",
    "  * Neural networks. The parameters are the network weights. \n",
    "  * Decision tree. Parameters are what to split on and where. This is a case where thinking about the set of estimator $\\hat{f}$ itself (the tree) is actually easier than thinking about the parameters. \n",
    "  * Random Forest. Same parameters as decision trees but one set of parameter for each tree in the forest. \n",
    "  * ... \n",
    "\n",
    "Notice that in the above, there are still other choices to make: distributions for the factors in naive bayes; architecture / topology, depth, width for neural networks; maximum number of nodes or maximum depth for decision trees; total number of trees in random forest. \n",
    "These choices that are **made before training** are called **hyperparameters**.\n",
    "\n",
    "\n",
    "\n",
    "## Objective / Score / Loss / Error function\n",
    "Not every guess in the family of $\\{\\hat{f}\\}$ will be good and we need a way to measure how good a guess $\\hat{f}$ is. For example, \n",
    " * Square loss: $L(\\hat{f})(x) = (f(x) - \\hat{f}(x))^2$. \n",
    " * Absolute error: $L(\\hat{f})(x) = |f(x) - \\hat{f}(x)|$. \n",
    " * Cross-entropy\n",
    " * log-loss\n",
    "(some of the above can only applied for specific kinds of $f$, e.g. log-loss is when we are trying to predict probability). Wait, but we just say that we don't  have access to $f$! So we can't actually evaluate any of the above. But we do have access to data where $f(x_i)$ is **approximately** $y_i$. So we replace that with \n",
    "\n",
    "$$L(\\hat{f})(x_i) = (y_i - \\hat{f}(x))^2$$\n",
    "\n",
    "But we have lots of data points $\\{(x_i, y_i)\\}$, well, just sum them up!\n",
    "\n",
    "$$L(\\hat{f}) = \\sum_i  (y_i - \\hat{f}(x))^2$$\n",
    "\n",
    "and we have arrived at the **training error** for a particular guess $\\hat{f}$. \n",
    "\n",
    "But our guesses $\\hat{f}$ are actually parametrised by $\\theta$, so we can actually express training loss as a function of $\\theta$, \n",
    "\n",
    "$$\n",
    "L(\\hat{f}_\\theta) = L(\\theta) = \\sum_i (y_i - \\hat{f}_\\theta(x))^2\n",
    "$$\n",
    "\n",
    "\n",
    "## Training / fitting\n",
    "With what we have set up above, training a model translate to \n",
    "> Find the best parameter $\\theta$ that minimises **training** loss $L(\\theta)$. \n",
    "\n",
    "This becomes purely a problem in **optimisation**. \n",
    "\n",
    "## Testing and generalisation error\n",
    "Once a good parameter and therefore a good $\\hat{f}$ is found, we can ask \n",
    "> \"Ok, so I know that $\\theta$ do well in my dataset, but how do I know that it will do well in the real world?\"\n",
    "\n",
    "Ideally we can just compare $\\hat{f}_\\theta$ against the truth $f$ and account for all possible value of $x$ (by taking expectation for example)\n",
    "$$\n",
    "L(\\hat{f}) = \\sum_{\\text{all possible values of $x$}} (\\hat{f}_\\theta(x) - f(x))^2 dx\n",
    "$$\n",
    "(I have ignored the fact that this should be an integral and you might want to weight against distribution of $x$ to make things converge). This is the **generalisation** score / loss / error. \n",
    "\n",
    "However, again, this whole exercise only occur because we don't know $f$!! What we can do is \n",
    " * Go and collect more data and try $\\hat{f}_\\theta$ on them and recalculate the loss. \n",
    " * Don't use all the data in $D_N$ for training, reserve some to simulate \"collecting more data later on\". (train-test split)\n",
    " * Do the previous train-test split, except we do that multiple times and then average the resulting scores. (cross validations)\n",
    "The hope is that the performance measure generated with these methods will be a good predictor for the actual performance \"in real life\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57575f16",
   "metadata": {},
   "source": [
    "----\n",
    "# Demo implementation of how to do sampling with replacement using different probability density\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6fe3b7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def sample_one(probabilities):\n",
    "    \"\"\"\n",
    "    Generate a single sample with probability density specified by\n",
    "      `probabilities = [p1, p2, ..., pN]`\n",
    "    We will return an index `i` between 0 and N-1 inclusive.\n",
    "    \"\"\"\n",
    "    assert np.sum(probabilities) # just checking this is an honest probability density\n",
    "    \n",
    "    N = len(probabilities) # size of the sample space\n",
    "    cummulative_density = np.cumsum(probabilities) # compute CDF\n",
    "    cummulative_density = np.concatenate([[0], cummulative_density]) # prepend 0 to the list\n",
    "    random_seed = np.random.rand() # generate a single random number between 0 and 1 uniformly.\n",
    "    \n",
    "    # Look for the index i such that CDF[i -1] <= random_seed <= CDF[i]\n",
    "    for i in range(N):\n",
    "        if cummulative_density[i] < random_seed <= cummulative_density[i + 1]:\n",
    "            return i # break and return when found\n",
    "    #return N -1 # if loop complete it must have fall on the last interval\n",
    "\n",
    "\n",
    "def sample(n, items, probabilities):\n",
    "    \"\"\"For pedagogy only. Inefficient implementation\"\"\"\n",
    "    samples = []\n",
    "    for _ in range(n):\n",
    "        index = sample_one(probabilities)\n",
    "        samples.append(items[index])\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9839cfd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empirical probabilities:\n",
      "Probability of oberving a = 0.1208\n",
      "Probability of oberving b = 0.1248\n",
      "Probability of oberving c = 0.505\n",
      "Probability of oberving d = 0.2494\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "items = ['a', 'b', 'c', 'd']\n",
    "\n",
    "probabilities = [1/8, 1/8, 4/8, 2/8]\n",
    "num_samples = 10000\n",
    "samples = sample(num_samples, items, probabilities)\n",
    "\n",
    "plt.hist(sorted(samples))\n",
    "print(f\"Empirical probabilities:\")\n",
    "a, b = np.unique(samples,  return_counts=True)\n",
    "for a, b in zip(*np.unique(samples,  return_counts=True)):\n",
    "    print(f\"Probability of oberving {a} = {b / num_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210f5d3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
