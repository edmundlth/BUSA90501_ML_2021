{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4411832",
   "metadata": {},
   "source": [
    "----\n",
    "# XGBoost practice\n",
    "---\n",
    "\n",
    "\n",
    "### Dataset\n",
    "We will again use the [pima indian diabetes dataset](pima-indians-diabetes.csv), whose description is [here](http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.names), which records measurements about several hundred patients and an indication of whether or not they tested positive for diabetes (the class label).  The classification is therefore to predict whether a patient will test positive for diabetes, based on the various measurements.\n",
    "\n",
    "The dataset has been downloaded as a csv file in the current directory. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af413966",
   "metadata": {},
   "source": [
    "## Exercise 0: prelude\n",
    "Install and import necessary modules and functions including: \n",
    " * pandas for loading and parsing data. \n",
    " * `cross_val_score` from sklearn to do cross validation automatically. \n",
    " * `xgboost` because we are using this model today. \n",
    "\n",
    "Load the pima data into a pandas dataframe. Do some exploration to gain some understanding of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b03152a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78a0f37c",
   "metadata": {},
   "source": [
    "## Exercise 1: Define `xgboost` model\n",
    "Define an `xgboost` model for our data. Since we are doing (binary) classification, we will use `XGBClassifier`. \n",
    "Do: \n",
    " * Read the docs:\n",
    "   * [Get started](https://xgboost.readthedocs.io/en/latest/get_started.html)\n",
    "   * [XGBClassifier]\n",
    " * Take note on the various parameters.\n",
    " * Instantiate the classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abb5cbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f47fb0f",
   "metadata": {},
   "source": [
    "### Performance evaluation\n",
    "a) Report the AUC of an XGBoost classifier on the dataset using 10 fold cross validation.\n",
    "\n",
    "b) Manually vary the following parameters and observe the effect on AUC of varying the following parameters for XGBoost? \n",
    "\n",
    "    • max depth\n",
    "    • learning rate \n",
    "    • n estimators \n",
    "    • gamma \n",
    "    • min child weight \n",
    "    • reg lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24386b0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45d118ef",
   "metadata": {},
   "source": [
    "## Exercise 2: Effect of hyperparameters\n",
    "The following example illustrates how one can explore the resulting performance of a classifier a we vary its   hyperparameter (`max_depth` in this case) by plotting `parameter vs performance` on a graph. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b01d176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a range of the hyperparameter. \n",
    "max_depths = range(1,15,2)\n",
    "\n",
    "# a list to accumulate the performance measure. We use accuracy for example. Change it as you wish.\n",
    "performance_scores=[]\n",
    "\n",
    "# Define the model. \n",
    "xgboostmodel = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    eta=0.3,\n",
    "    min_child_weight=1,\n",
    "    max_depth=3, \n",
    "    # add the following to remove two depreciation warning\n",
    "    eval_metric='mlogloss', \n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "# In a loop, assign different hyperparameter to the model and record performance measure. \n",
    "for m_depth in max_depths:\n",
    "    # reassign the hyperparameter\n",
    "    xgboostmodel.max_depth = m_depth\n",
    "    \n",
    "    # Compute performance score (we use the mean of 10-fold cross validation with ROCAUC as score each time)\n",
    "    score = cross_val_score(xgboostmodel, data,classlabel,cv=10,scoring='roc_auc').mean()\n",
    "    \n",
    "    # print it out if you wish\n",
    "    #print(\"10-fold cross validation AUC= \",cross_val_score(xgboostmodel, data,classlabel,cv=10,scoring='roc_auc').mean())\n",
    "    performance_scores.append(score)\n",
    "\n",
    "plt.plot(max_depths, performance_scores)\n",
    "plt.xlabel('Max-Depth', fontsize=16)\n",
    "plt.ylabel('AUC', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab58ab9",
   "metadata": {},
   "source": [
    "## Your turn\n",
    "Repeat the above for different hyperparameters and perhaps different performance measures (AUC, accuracy, f1 etc...)\n",
    "\n",
    "Do it at least for the following hyperparameters:  \n",
    "\n",
    "`eta [default=0.3]`\n",
    "\n",
    "    - Analogous to learning rate in GBM\n",
    "    - Makes the model more robust by shrinking the weights on each step\n",
    "    - Typical final values to be used: 0.01-0.2\n",
    "    \n",
    "    \n",
    "`n_estimators` \n",
    "(covered before)\n",
    "\n",
    "\n",
    "\n",
    "`min_child_weight [default=1]`\n",
    "    - Defines the minimum sum of weights of all observations required in a child.\n",
    "    - This refers to min “sum of weights” of observations.\n",
    "    - Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n",
    "    - Too high values can lead to under-fitting hence, it should be tuned using CV.\n",
    "    \n",
    "** stop trying to split once your sample size in a node goes below a given threshold.\n",
    "\n",
    "\n",
    "\n",
    "`gamma [default=0]`\n",
    "    - A node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the minimum loss reduction required to make a split.\n",
    "    - Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n",
    "\n",
    "** The complexity cost by introducing additional leaf\n",
    "\n",
    "\n",
    "`lambda [default=1]`\n",
    "    - L2 regularization term on weights (analogous to Ridge regression)\n",
    "    - This used to handle the regularization part of XGBoost. Though many data scientists don’t use it often, it should be explored to reduce overfitting.\n",
    "    \n",
    "    \n",
    "`max_leaf_nodes`\n",
    "    - The maximum number of terminal nodes or leaves in a tree.\n",
    "    - Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.\n",
    "    - If this is defined, GBM will ignore max_depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cea4ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc75fa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a8f206",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b8d7088",
   "metadata": {},
   "source": [
    "# Exercise 3: Grid Search \n",
    "Sklearn provides `GridSearchCV` to systematically search for a good **combination** of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f7f5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "xgboostmodel = XGBClassifier(\n",
    "    n_estimators=50,\n",
    "    eta=0.3,\n",
    "    min_child_weight=1,\n",
    "    max_depth=3, \n",
    "    # add the following to remove two depreciation warning\n",
    "    eval_metric='mlogloss', \n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "n_estimators = [50,100,200,300]\n",
    "max_depth = [2,4,6,8,10]\n",
    "tuned_parameters = dict(max_depth=max_depth, n_estimators=n_estimators)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, random_state=7, shuffle=True)\n",
    "\n",
    "clf = GridSearchCV(xgboostmodel, tuned_parameters, cv=kfold, scoring='roc_auc')\n",
    "\n",
    "clf.fit(data,classlabel)\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print(clf.best_params_)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
