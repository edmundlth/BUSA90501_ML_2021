{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# XGBoost practice\n",
    "---\n",
    "\n",
    "\n",
    "### Dataset\n",
    "We will again use the [pima indian diabetes dataset](pima-indians-diabetes.csv), whose description is [here](http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.names), which records measurements about several hundred patients and an indication of whether or not they tested positive for diabetes (the class label).  The classification is therefore to predict whether a patient will test positive for diabetes, based on the various measurements.\n",
    "\n",
    "The dataset has been downloaded as a csv file in the current directory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 0: prelude\n",
    "Install and import necessary modules and functions including: \n",
    " * pandas for loading and parsing data. \n",
    " * `cross_val_score` from sklearn to do cross validation automatically. \n",
    " * `xgboost` because we are using this model today. \n",
    "\n",
    "Load the pima data into a pandas dataframe. Do some exploration to gain some understanding of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Answer to 1a) here\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "# from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##load in the data\n",
    "data_filepath = '../test_data/pima-indians-diabetes.csv'\n",
    "pima=pd.read_csv(data_filepath,encoding = 'ISO-8859-1')\n",
    "\n",
    "##get just the features\n",
    "data=pima[['numpregnant','plasma','blood pressure','sf-thickness','serum-insulin','BMI','pedigree-function','age']]\n",
    "\n",
    "##get just the class labels\n",
    "classlabel=pima['has_diabetes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#conda install -c conda-forge py-xgboost\n",
    "from xgboost import XGBClassifier\n",
    "xgboostmodel = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    eta=0.3,\n",
    "    min_child_weight=1,\n",
    "    max_depth=3, \n",
    "    # add the following to remove two depreciation warning\n",
    "    eval_metric='mlogloss', \n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "xgboostmodel.fit(data, classlabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Define `xgboost` model\n",
    "Define an `xgboost` model for our data. Since we are doing (binary) classification, we will use `XGBClassifier`. \n",
    "Do: \n",
    " * Read the docs:\n",
    "   * [Get started](https://xgboost.readthedocs.io/en/latest/get_started.html)\n",
    "   * [XGBClassifier]\n",
    " * Take note on the various parameters.\n",
    " * Instantiate the classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation\n",
    "a) Report the AUC of an XGBoost classifier on the dataset using 10 fold cross validation.\n",
    "\n",
    "b) Manually vary the following parameters and observe the effect on AUC of varying the following parameters for XGBoost? \n",
    "\n",
    "    • max depth\n",
    "    • learning rate \n",
    "    • n estimators \n",
    "    • gamma \n",
    "    • min child weight \n",
    "    • reg lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"10-fold cross validation AUC= \",cross_val_score(xgboostmodel, data,classlabel,cv=10,scoring='roc_auc').mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Effect of hyperparameters\n",
    "The following example illustrates how one can explore the resulting performance of a classifier a we vary its   hyperparameter (`max_depth` in this case) by plotting `parameter vs performance` on a graph. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define a range of the hyperparameter. \n",
    "max_depths = range(1,15,2)\n",
    "\n",
    "# a list to accumulate the performance measure. We use accuracy for example. Change it as you wish.\n",
    "performance_scores=[]\n",
    "\n",
    "# Define the model. \n",
    "xgboostmodel = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    eta=0.3,\n",
    "    min_child_weight=1,\n",
    "    max_depth=3, \n",
    "    # add the following to remove two depreciation warning\n",
    "    eval_metric='mlogloss', \n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "# In a loop, assign different hyperparameter to the model and record performance measure. \n",
    "for m_depth in max_depths:\n",
    "    # reassign the hyperparameter\n",
    "    xgboostmodel.max_depth = m_depth\n",
    "    \n",
    "    # Compute performance score (we use the mean of 10-fold cross validation with ROCAUC as score each time)\n",
    "    score = cross_val_score(xgboostmodel, data,classlabel,cv=10,scoring='roc_auc').mean()\n",
    "    \n",
    "    # print it out if you wish\n",
    "    #print(\"10-fold cross validation AUC= \",cross_val_score(xgboostmodel, data,classlabel,cv=10,scoring='roc_auc').mean())\n",
    "    performance_scores.append(score)\n",
    "\n",
    "plt.plot(max_depths, performance_scores)\n",
    "plt.xlabel('Max-Depth', fontsize=16)\n",
    "plt.ylabel('AUC', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Booster Parameters\n",
    "2) eta [default=0.3]\n",
    "\n",
    "    - Analogous to learning rate in GBM\n",
    "    - Makes the model more robust by shrinking the weights on each step\n",
    "    - Typical final values to be used: 0.01-0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "l_rates = np.arange(0.01,0.2,0.01)\n",
    "accuracies=[]\n",
    "for l_r in l_rates:\n",
    "    xgboostmodel.eta = l_r\n",
    "\n",
    "#     xgboostmodel = XGBClassifier(n_estimators=100,eta=l_r,min_child_weight=1,max_depth=7)\n",
    "\n",
    "    #print(\"10-fold cross validation AUC= \",cross_val_score(xgboostmodel, data,classlabel,cv=10,scoring='roc_auc').mean())\n",
    "        \n",
    "    accuracies.append(cross_val_score(xgboostmodel, data,classlabel,cv=10,scoring='roc_auc').mean())\n",
    "\n",
    "plt.plot(l_rates,accuracies)\n",
    "plt.xlabel('Learning Rate', fontsize=16)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Booster Parameters\n",
    "3) n_estimators (covered before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "n_estimator = range(50, 500, 50)\n",
    "\n",
    "auc=[]\n",
    "for nt in n_estimator:\n",
    "    xgboostmodel.n_estimators = nt\n",
    "    \n",
    "#     xgboostmodel = XGBClassifier(n_estimators=nt,eta=0.3,min_child_weight=3,max_depth=2)\n",
    "    \n",
    "    cv_score = cross_val_score(xgboostmodel, data,classlabel,cv=10,scoring='roc_auc').mean()\n",
    "    auc.append(cv_score)\n",
    "\n",
    "plt.plot(n_estimator,auc)\n",
    "plt.xlabel('n_estimators', fontsize=16)\n",
    "plt.ylabel('AUC', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Booster Parameters\n",
    "4) min_child_weight [default=1]\n",
    "    - Defines the minimum sum of weights of all observations required in a child.\n",
    "    - This refers to min “sum of weights” of observations.\n",
    "    - Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n",
    "    - Too high values can lead to under-fitting hence, it should be tuned using CV.\n",
    "    \n",
    "** stop trying to split once your sample size in a node goes below a given threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Min_child_weight = range(1,15)\n",
    "\n",
    "auc=[]\n",
    "\n",
    "for mcw in Min_child_weight:\n",
    "    xgboostmodel.min_child_weight = mcw\n",
    "#     xgboostmodel = XGBClassifier(n_estimators=75,eta=0.3,min_child_weight=mcw,max_depth=2, gamma=1.5)\n",
    "    \n",
    "    cv_score = cross_val_score(xgboostmodel, data,classlabel,cv=10,scoring='roc_auc').mean()\n",
    "    auc.append(cv_score)\n",
    "\n",
    "plt.plot(Min_child_weight,auc)\n",
    "plt.xlabel('Min_child_weight', fontsize=16)\n",
    "plt.ylabel('AUC', fontsize=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Booster Parameters\n",
    "5) gamma [default=0]\n",
    "    - A node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the minimum loss reduction required to make a split.\n",
    "    - Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n",
    "\n",
    "** The complexity cost by introducing additional leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "gamma = np.arange(0, 1, 0.01)\n",
    "\n",
    "auc=[]\n",
    "for gm in gamma:\n",
    "    xgboostmodel.gamma = gm\n",
    "#     xgboostmodel = XGBClassifier(n_estimators=75,eta=0.3,min_child_weight=3,max_depth=2, gamma=gm)\n",
    "    \n",
    "    cv_score = cross_val_score(xgboostmodel, data,classlabel,cv=10,scoring='roc_auc').mean()\n",
    "    auc.append(cv_score)\n",
    "\n",
    "plt.plot(gamma,auc)\n",
    "plt.xlabel('gamma', fontsize=16)\n",
    "plt.ylabel('AUC', fontsize=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Booster Parameters\n",
    "6) lambda [default=1]\n",
    "    - L2 regularization term on weights (analogous to Ridge regression)\n",
    "    - This used to handle the regularization part of XGBoost. Though many data scientists don’t use it often, it should be explored to reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reg_lambda = [1e-5,1e-3, 1e-4 ,1e-2, 0.1, 0.2, 0.5, 0.9, 1,5,10,20,50, 100]\n",
    "\n",
    "auc=[]\n",
    "\n",
    "for regl in reg_lambda:\n",
    "    xgboostmodel.reg_lambda = regl\n",
    "    \n",
    "#     xgboostmodel = XGBClassifier(n_estimators=75,eta=0.3,min_child_weight=5,max_depth=2, gamma=1.5, reg_lambda = regl)\n",
    "    \n",
    "    cv_score = cross_val_score(xgboostmodel, data,classlabel,cv=10,scoring='roc_auc').mean()\n",
    "    auc.append(cv_score)\n",
    "\n",
    "plt.plot(reg_lambda,auc)\n",
    "plt.xlabel('reg_lambda', fontsize=16)\n",
    "plt.ylabel('AUC', fontsize=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search for Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "xgboostmodel = XGBClassifier(\n",
    "    n_estimators=50,\n",
    "    eta=0.3,\n",
    "    min_child_weight=1,\n",
    "    max_depth=3, \n",
    "    # add the following to remove two depreciation warning\n",
    "    eval_metric='mlogloss', \n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "# xgboostmodel = XGBClassifier(n_estimators=nt,eta=0.3,min_child_weight=3,max_depth=2)\n",
    "\n",
    "n_estimators = [50,100,200,300]\n",
    "max_depth = [2,4,6,8,10]\n",
    "tuned_parameters = dict(max_depth=max_depth, n_estimators=n_estimators)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, random_state=7, shuffle=True)\n",
    "\n",
    "clf = GridSearchCV(xgboostmodel, tuned_parameters, cv=kfold, scoring='roc_auc')\n",
    "\n",
    "clf.fit(data,classlabel)\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print(clf.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
