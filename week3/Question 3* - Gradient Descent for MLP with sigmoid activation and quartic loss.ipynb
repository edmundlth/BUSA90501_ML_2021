{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85ba6936",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# import ml models here\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad7395a",
   "metadata": {},
   "source": [
    "---\n",
    "# Gradient Descent for MLP with sigmoid activation and quartic loss\n",
    "\n",
    "Question statement: \\\n",
    "Develop the training logic for a **SINGLE HIDDEN LAYER MLP** classifier using the sigmoid function as activation function and the sum of errors to the power of four (instead of the sum of squared errors) as the error function.\n",
    "\n",
    "---\n",
    "# Messy details of the netwok itself\n",
    "Our goal is to define a neural network that defines a **function** \n",
    " * taking $d$-dimensional input, i.e. each input $x = (x_1, \\dots, x_d) = (x_i)_{i = 1, \\dots, d}$ is a $d$-dimensional vector. \n",
    " * and output $n$ dimensional vector\n",
    "Meaning, out neural network here would be a function of the following form.\n",
    "$$\n",
    "f: \\mathbb{R}^d \\to \\mathbb{R}^n\\\\\n",
    "f(\\mathbf{x}) = f(x_1, \\dots, x_d) = \\left(f_1(\\mathbf{x}), \\dots, f_n(\\mathbf{x})\\right)\n",
    "$$\n",
    "\n",
    "**Input layer to hidden layer** \n",
    "\n",
    "Since we have $d$-dimensional input, the input layer has $d$ nodes. Now, since we are only implementing single-layer feedforward network, we only have 1-hidden layer with a size of $M$. So, each node in the hidden layer is taking all $d$ values coming from the input layer and computing the function\n",
    "$$ \\text{HiddenNode}_j(x_1, \\dots, x_d) = \\sigma\\left(\\sum_{i = 1}^d W^{(1)}_{ji}x_i + b^{(1)}_{j}\\right)$$\n",
    "Note:\n",
    " * The question asked us to use sigmoid activation function, i.e. \n",
    " $$\\sigma(x) = \\frac{1}{1 - e^{-x}}$$\n",
    " * Note that the activation function only cares about a **scalar** input!! It takes in one number and output one number. \n",
    " * the superscript in $W^{(1)}, b^{(1)}$ is to remind ourselves that we are going from layer $0$ (the input) to layer $1$. \n",
    " * I am fixing the indexing variable here so that you know what variable correspond to which layer. \n",
    " * $i$ is the index for the input layer, $i = 1, \\dots d$. \n",
    " * $j$ is the index for the hidden layer, $j = 1, \\dots M$. So the above expression is repeated with different weights $W_{ji}$ and biases $b_{j}$ for each $j = 1, \\dots, M$. \n",
    " * We use capital letter $W$ because it is going to look a whole lot like a marix later on. \n",
    " * The reason the index $ji$ is labelled in that order is also because they correspond to the row and column index of the matrix later on. \n",
    " \n",
    "\n",
    "**Hidden layer to output layer**  \n",
    "\n",
    "Since the output dimension is $n$, the output layers have $n$ nodes, each taking input from the $M$ output from the hidden layer, the expression for each of the output nodes $f_k$ is given by\n",
    "$$\n",
    "f_k = \\sigma\\left(\\sum_{j = 1}^M W^{(2)}_{kj}\\text{HiddenNode}_j + b^{(2)}_{k} \\right)\n",
    "$$\n",
    "\n",
    "Notes: \n",
    " * Yes, this is the same $f_1(\\mathbf{x}), \\dots, f_n(\\mathbf{x})$ from the very begining, but note that the expression above only depends on $\\text{HiddenNode}_1, \\dots \\text{HiddenNode}_M$, which in turn depends on $x_1, \\dots, x_d$. \n",
    " * $k$ indexes the output nodes, $k = 1, \\dots n$ with $n$, again, being the output dimension. \n",
    "\n",
    "\n",
    "\n",
    "**The full unnecessarily messy functional form**\n",
    "With the definition above, this single-layer-sigmoid neural network is just a function $f = (f_1, \\dots, f_n)$, with expression \n",
    "$$\n",
    "f_k(x_1, \\dots, x_d) = \\sigma\\left(\\sum_{j = 1}^M \n",
    "    W^{(2)}_{kj}\\sigma\\left(\\sum_{i = 1}^d W^{(1)}_{ji}x_i + b^{(1)}_{j}\\right) + b^{(2)}_{k} \n",
    "\\right) \\\\\n",
    "$$\n",
    "\n",
    "<font color=\"red\">\n",
    "You can see why we choose to draw networks than writing out the functional form.\n",
    "</font>\n",
    "\n",
    "\n",
    "## Simplification\n",
    "Just to simplify the expression for neural network we introduce the following convention  \n",
    "\n",
    "**Elementwise notation**  \n",
    "When you use `numpy.log([1, 2, 3])`, numpy internally does `[log(1), log(2), log(3)]` for you, meaning the function is applyed **element-by-element**. The reason is that instead of doing `for elem in array: log(elem)` in python, the loop actually occur in $C$ which is a lot faster. This notation is common and worth knowing: when you have function $\\sigma(x)$ that is suppose to take one input and give one output, but the written input is a vector (or array of any dimension) it is understood that we mean apply the function **element-by-element** and output an array of the same dimension. \n",
    "\n",
    "**Matrix multiplication**  \n",
    "Notice that the sum $\\sum_{i = 1}^d W_{ji}x_i$ is the same as the dot-product of the $j^{th}$ of the matrix $W$ with the vector $\\mathbf{x} = (x_1, \\dots, x_d)$. So, in vector form, we have \n",
    "$$\n",
    "\\text{Vector output $y_j$ for $j = 1 \\dots k$} = \\left(\\begin{matrix}\n",
    "y_1 \\\\\n",
    "\\vdots \\\\\n",
    "y_k\n",
    "\\end{matrix}\\right) = W\\mathbf{x}\n",
    "$$\n",
    "\n",
    "\n",
    "**Absorbing the bias term in to the weights**  \n",
    "The bias term is a little confusing for me. For example is the bias applied before or after the activation? Both should work in practice but I think the consensus is that it is applied before the activation (the expression above is with this assumption). With that in mind, we can do a trick that simplifies the presentation even more. \n",
    "\n",
    "We will use the same trick people use in [linear regression](https://en.wikipedia.org/wiki/Linear_regression#Formulation). We add a $0^{th}$ dimension to each layer (including input layer), which is a constant vector of ones. This amounts to adding another node to the input layer so that it becomes $(1, x_1, \\dots, x_d)$ instead of just $(x_1, \\dots, x_d)$. With this we can rewrite the \n",
    "\n",
    "$$\n",
    "\\left(\n",
    "\\begin{matrix}\n",
    "\\sum_{i = 1}^d W_{1i}x_i + b_{1} \\\\\n",
    "\\dots \\\\\n",
    "\\sum_{i = 1}^d W_{ki}x_i + b_{k}\n",
    "\\end{matrix}\n",
    "\\right) = \n",
    "\\left(\n",
    "\\begin{matrix}\n",
    "W_{11} & W_{12} & \\dots & W_{1d} \\\\\n",
    "W_{21} & W_{22} & \\dots & W_{2d} \\\\\n",
    "\\ddots & & \\\\\n",
    "\\dots & W_{ji} & \\dots \\\\\n",
    "W_{k1} & & \\dots & W_{kd}\n",
    "\\end{matrix}\n",
    "\\right)\n",
    "\\left(\\begin{matrix} x_1 \\\\ \\vdots \\\\ x_d \\end{matrix}\\right) + \\left(\\begin{matrix} b_1 \\\\ \\vdots \\\\ b_d \\end{matrix}\\right) = \n",
    "\\left(\n",
    "\\begin{matrix}\n",
    "b_1 & W_{11} & W_{12} & \\dots & W_{1d} \\\\\n",
    "b_2 & W_{21} & W_{22} & \\dots & W_{2d} \\\\\n",
    "\\ddots & & \\\\\n",
    "\\dots & W_{ji} & \\dots \\\\\n",
    "b_k & W_{k1} & & \\dots & W_{kd}\n",
    "\\end{matrix}\n",
    "\\right)\n",
    "\\left(\\begin{matrix}1 \\\\ x_1 \\\\ \\vdots \\\\ x_d \\end{matrix}\\right) = \\widetilde{W}\\widetilde{x}\n",
    "$$\n",
    "where we absorbed the bias vector into the weight matrix $W$ and $\\widetilde{x}$ operation simply add initial element of 1 into a vector. We index those initial addition with the index $0$.\n",
    "\n",
    "\n",
    "**Pulling together**  \n",
    "\n",
    "So, with the addition of the \"bias node\" (i.e. a node that just output 1) in the input layer and the hidden-layer, and using elementwise notation for the activation function, we can rewrite \n",
    "$$\n",
    "f(x) = \\sigma(W^{(2)}\\widetilde{\\sigma(W^{(1)}\\tilde{x})})\n",
    "$$\n",
    "where the bias vectors for both layers $b^{(1)}$ and $b^{(2)}$ has been absorbed into the weight matrix $W^{(1)}$ and $W^{(2)}$ as the respective layers. \n",
    "\n",
    "\n",
    "\n",
    "**Shorthand for the full network**  \n",
    "\n",
    "We shall write our network as $f_W(x)$ where $W$ include both $W^{(1)}$ and $W^{(2)}$ (or more if this is multi-layered) with the bias vectors absorbed. This is to make the explicit that an network prediction on an input $x$ is dependent on the set of **network parameters** $W$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bee2fb",
   "metadata": {},
   "source": [
    "----\n",
    "# Loss function\n",
    "---\n",
    "Instead of squared-error, we are prompted to use quartic $x^4$ error. \n",
    "\n",
    "Let's expand definitions to see what that means: \n",
    " 1. Given an input $x$. Remember that we have $d$ dimensional input so it is really $x = (x_1, \\dots, x_d)$. \n",
    " 2. with the true output $y(x)$, Remember that we have $n$ dimensional output so it is really $y = (y_1, \\dots, y_n)$.\n",
    " 3. We can compute the network prediction by just passing $x$ through our network $f_W$, giving $f_W(x)$. Again, don't forget that our output is $n$-dimensional, so it is really $f_W(x) = (f_W(x)_1, \\dots, f_W(x)_n)$. \n",
    " 4. We want out error to be proportional to the difference between $y$ and $f(x)$ to the 4th power. We simply sum the difference component-wise and raise it to the 4th power:\n",
    " $$\n",
    " L_W(x) = \\frac{1}{4} \\sum_{k = 1}^n (y_k(x) - f_W(x)_k)^4\n",
    " $$\n",
    "\n",
    "Note: \n",
    " * I am using the same indexing variable $k$ for the output dimension which is the same as the size (number of nodes) in the final output layer. \n",
    " * the $1/4$ is added purely to remove the constant later when we differentiate $x^4$. Scaling the loss by a constant does not change where the minimum occurs. \n",
    " * This is the loss for a single input $x$. So if we are doing online learning, this is enough. \n",
    " * If we are doing batch or minibatch gradient descent, we simply take the batch average: \n",
    " $$ L_W(\\{X^1, X^2, \\dots, X^b, \\dots, X^B\\}) = \\frac{1}{B}\\sum_{b = 1}^B L_W(X_b)$$\n",
    " where we use $b$ as the batch sample index with $B$ as the batch size. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cff100a",
   "metadata": {},
   "source": [
    "----\n",
    "# Gradient of the Loss Function\n",
    "----\n",
    "Since we aim to use SGD to find the minima of the loss function, we need to compute its gradient **with respect to the network parameters**\n",
    "$$\n",
    "\\nabla_W L_W(x) \n",
    "$$\n",
    "which tells us the direction of greatest *ascent* when we are at a specific parameter $W$ with $-\\nabla_W L_W(x)$ being the direction (in parameter space) where we want to step to get the most decrease in loss. This is really just an exercise in using chain-rule and carefull book keeping of various indices. \n",
    "\n",
    "Note: \n",
    " * During calculation of the gradient, the input $x$ or the batch of inputs $\\{X^1, \\dots, X^B\\}$ is **FIXED** and \n",
    " * we only care about minimising loss with respect to the **network parameters**, i.e. the thing that is changing during training time. We are searching for the best(-ish) parameters **GIVEN** that we know the output for the batch of inputs. \n",
    "\n",
    "---\n",
    "## Ok, here goes the Gory details...\n",
    "First, lets note that, since taking derivative commutes with taking (finite) average, \n",
    "$$ \\partial_{w} L_W(\\{X^1, X^2, \\dots, X^b, \\dots, X^B\\}) = \\frac{1}{B}\\sum_{b = 1}^B \\partial_{w} L_W(X_b)$$\n",
    "where $\\partial_w = \\frac{\\partial}{\\partial w}$ is the partial derivative with respect to the variable $w$. So we really only need to compute the gradient for the per-input loss $L_W(x)$. \n",
    "\n",
    "Lets use the notation $\\partial_{ji} = \\partial_{w_{ji}}$ which is the partial derivative with respect to network weight $W^{(1)}_{ji}$ where (recall that) $i = 0, 1 \\dots d$ and $j = 0, 1 \\dots M$, where we absorbed bias terms into the $0^{th}$ index. Similarly for the second layer $\\partial_{kj}$ is for the network weight $W^{(2)}_{kj}$, $k = 1 \\dots n$. With this notation the full gradient is just the vector\n",
    "$$\n",
    "\\nabla_W L_W(x) = (\\partial_{ji} L_W, \\partial_{kj} L_W. \n",
    "$$\n",
    "Not the best notation I've come up with but yea .... \n",
    "\n",
    "\n",
    "\n",
    "So here goes the chain rule 😭 and lets to the weights for the final layer first... \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\partial_{kj} L_W(x) \n",
    "& = \\frac{1}{4} \\sum_{k' = 1}^n \\partial_{kj}(y_k(x) - f_W(x)_{k'})^4 \\\\\n",
    "& = \\sum_{k' = 1}^n (y_{k'}(x) - f_W(x)_{k'})^3 \\partial_{kj}f_W(x)_{k'} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "Notice that I have to rename the summation variable to $k'$ because that' different from $k$  but both are indexing the output dimension. Fortunately, the variable $w_{kj}$ only appears in terms where $k' = k$, and so alal other terms with $k' \\neq k$ have zero $\\partial_{kj}$-derivative, and we have \n",
    "$$\n",
    "\\partial_{kj} L_W(x) = (y_k(x) - f_W(x)_k)^3 \\partial_{kj}f_W(x)_{k} \\\\\n",
    "$$\n",
    "So, as expected we need to know the gradient with respect to the network itself $\\partial_{kj}f_W(x)$. \n",
    "\n",
    "\n",
    "And that's another chain rule 😭... (we use $h_j$ for the output of $j^{th}$ hidden node (note that it is a scalar)\n",
    "$$\n",
    "\\begin{align*}\n",
    "& \\partial_{kj}f_W(x)_{k} \\\\\n",
    "&= \\partial_{kj} \\sigma\\left(\\sum_{j' = 0}^M W^{(2)}_{kj'}h_{j'} \\right) \\tag*{notice index starts from 0}\\\\\n",
    "&= \\sigma'\\left(\\sum_{j' = 0}^M W^{(2)}_{kj'}h_{j'}\\right)\\, \n",
    "   \\partial_{kj}\\sum_{j' = 0}^M W^{(2)}_{kj'}h_{j'} \\tag*{chain rule} \\\\\n",
    "&= \\sigma'\\left(\\sum_{j' = 0}^M W^{(2)}_{kj'}h_{j'}\\right)\\, \n",
    "   \\partial_{kj} W^{(2)}_{kj}h_{j} \\tag*{non-$kj$ terms vanishes} \\\\\n",
    "&= \\sigma'\\left(\\sum_{j' = 0}^M W^{(2)}_{kj'}H_{j'}\\right)\\, h_{j}\\tag*{$\\partial_{kj}W_{kj} =1$} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So far we have not restrict ourselve to a particular activation function. As long as we know the derivative $\\sigma'$ we are good. For sigmoid activation, we have (exercise) $\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$. So collecting all these together we have \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\partial_{kj} L_W(x) \n",
    "&= (y_k(x) - f_W(x)_k)^3 \\partial_{kj}f_W(x)_{k} \\\\\n",
    "&= (y_k(x) - f_W(x)_k)^3 \\sigma'\\left(\\sum_{j' = 0}^M \n",
    "    W^{(2)}_{kj'}\\sigma\\left(\\sum_{i = 0}^d W^{(1)}_{j'i}x_i \\right)\n",
    "\\right)\\, \\sigma\\left(\\sum_{i = 0}^d W^{(1)}_{ji}x_i \\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and using elementwise notation\n",
    "$$\n",
    "\\partial^{(2)} L_W(x) = (y(x) - f_W(x))^3 \\sigma'\\left( \n",
    "    W^{(2)} \\sigma\\left(W^{(1)}x \\right)\n",
    "\\right)\\, \\sigma\\left( W^{(1)}x \\right)\n",
    "$$\n",
    "with $\\partial^{(2)}$ signalling that we are taking derivatie with respect to weights in $W^{(2)}$ only. Note that the output of the expression above is a vector of dimensin $n$ (output dimension). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91051603",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9f0a45ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 - np.exp(-x))\n",
    "\n",
    "def sigmoid_diff(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def ext(x):\n",
    "    # extend the vector x by prepending a 1.\n",
    "    return np.concatenate([[1], x])\n",
    "\n",
    "class SingleLayerSigmoidFeedForwardNetwork():\n",
    "    \n",
    "    def __init__(self, hidden_layer_size, input_dim=1, output_dim=1):\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.w1 = np.random.rand(input_dim + 1, hidden_layer_size)\n",
    "        self.w2 = np.random.rand(hidden_layer_size + 1, output_dim)\n",
    "        \n",
    "        self.activation = sigmoid\n",
    "        self.learning_rate = 0.0001\n",
    "    \n",
    "    def forward_pass(self, x):\n",
    "        h = sigmoid(ext(x) @ self.w1) # hidden layer outputs\n",
    "        network_output = sigmoid(ext(h) @ self.w2) \n",
    "        return network_output\n",
    "    \n",
    "    def loss(self, x, y):\n",
    "        pred = self.forward_pass(x)\n",
    "        return np.sum((y - pred)**2)/2\n",
    "    \n",
    "    def grad_layer2(self, x):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def grad_layer1(self, x):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def step(self, x):\n",
    "        self.w2 -= self.learning_rate * self.grad_layer2(x)\n",
    "        self.w1 -= self.learning_rate * self.grad_layer1(x)\n",
    "        return\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "860cc28e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1900991/3814933384.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1900991/1786916877.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw2\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_layer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw1\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_layer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1900991/1786916877.py\u001b[0m in \u001b[0;36mgrad_layer2\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgrad_layer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgrad_layer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x = np.array([3, 2, 5])\n",
    "clf = SingleLayerSigmoidFeedForwardNetwork(10, input_dim=x.shape[0])\n",
    "\n",
    "clf.forward_pass(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d2aafd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
