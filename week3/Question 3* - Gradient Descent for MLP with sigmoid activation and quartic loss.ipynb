{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6571d6a6",
   "metadata": {},
   "source": [
    "<font size=4 color=\"red\">\n",
    "    WARNING: This is for **Multi Layer Perceptron** (MLP) which is NOT the same as question 3 in week 3 worksheet because I MADE A MISTAKE. <br>\n",
    "    The worksheet asked for **PERCEPTRON** with sigmoid activation fuction and quartic (x^4) loss. <br> \n",
    "    For some reason I thought Perceptron is the same as MLP with single hidden layer, but **IT IS NOT**. \n",
    "    <br><br>\n",
    "    I messed up the terminology. Perceptron is more like fully connected feedforward network (a.k.a. MLP) without any hidden-layer at all, feeding input straight into an activation function.\n",
    "    <br><br>\n",
    "    This notebook (and during class) I was working with MLP with 1-hidden layer. I only realised my mistake later.\n",
    "    I sincerely apologise for the confusion. \n",
    "    <br><br>\n",
    "    See written solution for the answer for Perceptron (it's much much simpler...). \n",
    "    <br> \n",
    "    Hopefully this note might help with \n",
    "    <ul> \n",
    "        <li> understanding backprop, </li>\n",
    "        <li> the trick for absorbing bias into weights </li> \n",
    "        <li> Counting number of network parameters </li>\n",
    "        <li> the gory index-tracking and notation used to simplify it </li>\n",
    "        <li> implementing MLP in Python yourself </li>\n",
    "    </ul>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad7395a",
   "metadata": {},
   "source": [
    "---\n",
    "# Gradient Descent for MLP with sigmoid activation and quartic loss\n",
    "We wish to implement explicitly ourselves a fully connected feedforward network (a.k.a. Multi Layer Perceptron) with \n",
    " * $d$-dimensional inpuut, \n",
    " * $n$-dimensional output and \n",
    " * $L$ number of hidden layers each with $m_1, \\dots, m_L$ number of hidden units. \n",
    " * each neuron has activation function $\\sigma(x) = 1/ (1 - exp(x))$\n",
    " \n",
    "TODO: picture. \n",
    "\n",
    "---\n",
    "# Messy details of the netwok itself\n",
    "The neural network defines a **function** \n",
    " * taking $d$-dimensional input, i.e. each input $x = (x_1, \\dots, x_d) = (x_i)_{i = 1, \\dots, d}$ is a $d$-dimensional vector. \n",
    " * and output $n$ dimensional vector\n",
    "Meaning, out neural network here would be a function of the following form.\n",
    "$$\n",
    "f: \\mathbb{R}^d \\to \\mathbb{R}^n\\\\\n",
    "f(\\mathbf{x}) = f(x_1, \\dots, x_d) = \\left(f_1(\\mathbf{x}), \\dots, f_n(\\mathbf{x})\\right)\n",
    "$$\n",
    "\n",
    "**Input layer to first hidden layer** \n",
    "\n",
    "Since we have $d$-dimensional input, the input layer has $d$ nodes. So, each node in the first hidden layer is taking all $d$ values coming from the input layer. Let $H^1_{j}$ denote the $j^{th}$ neuron of the first input layer. Then, \n",
    "$$ H^1_i(x_1, \\dots, x_d) = \\sigma\\left(\\sum_{j = 1}^d W^{(1)}_{ij}x_i + b^{(1)}_{i}\\right)$$\n",
    "Note:\n",
    " * Note that the activation function only cares about a **scalar** input!! It takes in one number and output one number. Verify that the input to $\\sigma$ above is actually a scalar.  \n",
    " * the superscript in $W^{(l)}, b^{(l)}$ is to remind ourselves that we coming from layer $0$ (the input) into layer $1$. \n",
    " * I am using a fixed indexing variable naming here to keep track of what objects are being indexed: \n",
    "   * $l$ is the index for hidden layer $l = 0$ (i.e. the input layer) above and $l = 0, \\dots L$. \n",
    "   * $i$ is the index for the neurons in hidden layer $l + 1$, so $i = 1, \\dots, m_{l + 1}$. \n",
    "   * $j$ is the index for the neurons in hidden layer $l$, so $j = 1, \\dots, m_l$. We need this because the input to layer $l + 1$ depends (and only depends) on the neurons in the layer before it and the connecting weights $W^{(l)}_{ij}$. \n",
    " * We use capital letter $W$ because it is going to look a whole lot like a marix later on. \n",
    " * The reason the index $ij$ is labelled in that order is also because they correspond to the row and column index of the matrix later on. \n",
    "\n",
    "\n",
    "**Hidden Layer in between**   \n",
    "The equation for the hidden node at layer $l > 0$ generalise in a straightforward manner from the above:\n",
    "$$\n",
    "H^{l + 1}_i = \\sigma\\left(\\sum_{j = 1}^{m_l} W^{(l)}_{ij}H^{l}_i + b^{(l)}_{i}\\right)\n",
    "$$\n",
    "we only need to change the input $x$ into input from a previous layer $h^{l}$ and be careful of the bounds of the index $i = 1, \\dots, m_{l + 1}$ and $j = 1, \\dots, m_l$. \n",
    "\n",
    "\n",
    "**Hidden layer to output layer**  \n",
    "\n",
    "Since the output dimension is $n$, the output layers have $n$ nodes, each taking input from the $m_L$ outputs from the last hidden layer, the expression for each of the output nodes $f_i$, $i = 1, \\dots, n$ is given by\n",
    "$$\n",
    "f_i = \\sigma\\left(\\sum_{j = 1}^M W^{(L)}_{ij}H^{L}_j + b^{(L)}_{i} \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**The full unnecessarily messy functional form**  \n",
    "With the definition above, this MLP network is just a function $f = (f_1, \\dots, f_n)$, with the following complicated looking expression \n",
    "$$\n",
    "f_i(x_1, \\dots, x_d) \n",
    "= \\sigma\\left(\n",
    "    \\sum_{j_L = 1}^{m_L} W^{(L)}_{ij_L}\n",
    "        \\sigma\\left(\n",
    "            \\sum_{j_{L-1} = 1}^{m_{L-1}} W^{(L-1)}_{j_{L-1}i}\n",
    "                \\sigma\\left( \\dots\n",
    "                    \\sigma\\left(\\sum_{j_0 = 1}^{d} W^{(0)}_{j_1j_0}x_{j_0} + b^{(0)}_{j_1}\n",
    "                    \\right) + b^{(1)}_{j_2}\n",
    "                \\right) + \\dots\n",
    "            \\right) + b^{(L-1)}_{j_{L}} \n",
    "\\right) + b^{L}_i \\\\\n",
    "$$\n",
    "\n",
    "<font color=\"red\">\n",
    "You can see why we choose to draw networks rather than writing out the functional form. The network makes it clear what nodes depends on what weights. \n",
    "</font>\n",
    "\n",
    "\n",
    "## Simplification\n",
    "Just to simplify the expression for neural network we introduce the following convention  \n",
    "\n",
    "**Elementwise notation**  \n",
    "When you use `numpy.log([1, 2, 3])`, numpy internally does `[log(1), log(2), log(3)]` for you, meaning the function is applyed **element-by-element**. The reason is that instead of doing `for elem in array: log(elem)` in python, the loop actually occur in $C$ which is a lot faster. This notation is common and worth knowing: when you have function $\\sigma(x)$ that is suppose to take one input and give one output, but the written input is a vector (or array of any dimension) it is understood that we mean apply the function **element-by-element** and output an array of the same dimension. \n",
    "\n",
    "**Matrix multiplication**  \n",
    "Notice that the sum $\\sum_{i = 1}^d W_{ji}x_i$ is the same as the dot-product of the $j^{th}$ of the matrix $W$ with the vector $\\mathbf{x} = (x_1, \\dots, x_d)$. So, in vector form, we have \n",
    "$$\n",
    "\\text{Vector output $y_j$ for $j = 1 \\dots k$} = \\left(\\begin{matrix}\n",
    "y_1 \\\\\n",
    "\\vdots \\\\\n",
    "y_k\n",
    "\\end{matrix}\\right) = W\\mathbf{x}\n",
    "$$\n",
    "\n",
    "\n",
    "**Absorbing the bias term in to the weights**  \n",
    "The bias term is a little confusing for me. For example is the bias applied before or after the activation? Both should work in practice but I think the consensus is that it is applied before the activation (the expression above is with this assumption). With that in mind, we can do a trick that simplifies the presentation even more. \n",
    "\n",
    "We will use the same trick people use in [linear regression](https://en.wikipedia.org/wiki/Linear_regression#Formulation). We add a $0^{th}$ dimension to each layer (including input layer), which is a constant vector of ones. This amounts to adding another node to the input layer so that it becomes $(1, x_1, \\dots, x_d)$ instead of just $(x_1, \\dots, x_d)$. With this we can rewrite the \n",
    "\n",
    "$$\n",
    "\\left(\n",
    "\\begin{matrix}\n",
    "\\sum_{i = 1}^d W_{1i}x_i + b_{1} \\\\\n",
    "\\dots \\\\\n",
    "\\sum_{i = 1}^d W_{ki}x_i + b_{k}\n",
    "\\end{matrix}\n",
    "\\right) = \n",
    "\\left(\n",
    "\\begin{matrix}\n",
    "W_{11} & W_{12} & \\dots & W_{1d} \\\\\n",
    "W_{21} & W_{22} & \\dots & W_{2d} \\\\\n",
    "\\ddots & & \\\\\n",
    "\\dots & W_{ji} & \\dots \\\\\n",
    "W_{k1} & & \\dots & W_{kd}\n",
    "\\end{matrix}\n",
    "\\right)\n",
    "\\left(\\begin{matrix} x_1 \\\\ \\vdots \\\\ x_d \\end{matrix}\\right) + \\left(\\begin{matrix} b_1 \\\\ \\vdots \\\\ b_d \\end{matrix}\\right) = \n",
    "\\left(\n",
    "\\begin{matrix}\n",
    "b_1 & W_{11} & W_{12} & \\dots & W_{1d} \\\\\n",
    "b_2 & W_{21} & W_{22} & \\dots & W_{2d} \\\\\n",
    "\\ddots & & \\\\\n",
    "\\dots & W_{ji} & \\dots \\\\\n",
    "b_k & W_{k1} & & \\dots & W_{kd}\n",
    "\\end{matrix}\n",
    "\\right)\n",
    "\\left(\\begin{matrix}1 \\\\ x_1 \\\\ \\vdots \\\\ x_d \\end{matrix}\\right) = \\overline{W}\\overline{x}\n",
    "$$\n",
    "where we absorbed the bias vector into the weight matrix $W$ and $\\overline{x}$ operation simply add initial element of 1 into a vector. We index those initial addition with the index $0$.\n",
    "\n",
    "\n",
    "**Shorthand for MLP: Pulling together simplifications**  \n",
    "\n",
    "So, with the addition of the \"bias node\" (i.e. a node that just output 1) in the input layer and the hidden-layer, and using elementwise notation for the activation function, we can rewrite \n",
    "$$\n",
    "f(x) = \\sigma(W^{(L)}\\overline{\\sigma(W^{(L-1)}\\overline{\\sigma(\\dots (\\overline{\\sigma(W^{(0)}\\overline{x})}))})})\n",
    "$$\n",
    "where the bias vectors $b^{(l)}$ have been absorbed into the weight matrices $W^{(l)}$. \n",
    "\n",
    "\n",
    "We shall write our network as $f_W(x)$ where $W$ denote the full $W^{(l)}$ with the bias vectors absorbed. This is to make the explicit that an network prediction on an input $x$ is dependent on the set of **network parameters** $W$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bee2fb",
   "metadata": {},
   "source": [
    "----\n",
    "# Loss function\n",
    "---\n",
    "Instead of squared-error, we are prompted to use quartic $x^4$ error. \n",
    "\n",
    "Let's expand definitions to see what that means: \n",
    " 1. Given an input $x$. Remember that we have $d$ dimensional input so it is really $x = (x_1, \\dots, x_d)$. \n",
    " 2. with the true output $y(x)$, Remember that we have $n$ dimensional output so it is really $y = (y_1, \\dots, y_n)$.\n",
    " 3. We can compute the network prediction by just passing $x$ through our network $f_W$, giving $f_W(x)$. Again, don't forget that our output is $n$-dimensional, so it is really $f_W(x) = (f_W(x)_1, \\dots, f_W(x)_n)$. \n",
    " 4. We want out error to be proportional to the difference between $y$ and $f(x)$ to the 4th power. We simply sum the difference component-wise and raise it to the 4th power:\n",
    " $$\n",
    " L_W(x) = \\frac{1}{4} \\sum_{k = 1}^n (y_k(x) - f_W(x)_k)^4\n",
    " $$\n",
    "\n",
    "Note: \n",
    " * I am using the same indexing variable $k$ for the output dimension which is the same as the size (number of nodes) in the final output layer. \n",
    " * the $1/4$ is added purely to remove the constant later when we differentiate $x^4$. Scaling the loss by a constant does not change where the minimum occurs. \n",
    " * This is the loss for a single input $x$. So if we are doing online learning, this is enough. \n",
    " * If we are doing batch or minibatch gradient descent, we simply take the batch average: \n",
    " $$ L_W(\\{X^1, X^2, \\dots, X^b, \\dots, X^B\\}) = \\frac{1}{B}\\sum_{b = 1}^B L_W(X_b)$$\n",
    " where we use $b$ as the batch sample index with $B$ as the batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1fa552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7cff100a",
   "metadata": {},
   "source": [
    "----\n",
    "# Gradient of the Loss Function\n",
    "----\n",
    "Since we aim to use SGD to find the minima of the loss function, we need to compute its gradient **with respect to the network parameters**\n",
    "$$\n",
    "\\nabla_W L_W(x) \n",
    "$$\n",
    "which tells us the direction of greatest *ascent* when we are at a specific parameter $W$ with $-\\nabla_W L_W(x)$ being the direction (in parameter space) where we want to step to get the most decrease in loss. This is really just an exercise in using chain-rule and carefull book keeping of various indices. \n",
    "\n",
    "Note: \n",
    " * During calculation of the gradient, the input $x$ or the batch of inputs $\\{X^1, \\dots, X^B\\}$ is **FIXED** and \n",
    " * we only care about minimising loss with respect to the **network parameters**, i.e. the thing that is changing during training time. We are searching for the best(-ish) parameters **GIVEN** that we know the output for the batch of inputs. \n",
    "\n",
    "---\n",
    "## Ok, here goes the Gory details...\n",
    "First, lets note that, since taking derivative commutes with taking (finite) average, \n",
    "$$ \\frac{\\partial}{\\partial w} L_W(\\{X^1, X^2, \\dots, X^b, \\dots, X^B\\}) = \\frac{1}{B}\\sum_{b = 1}^B \\frac{\\partial}{\\partial w} L_W(X_b).$$\n",
    "So we really only need to compute the gradient for the per-input loss $L_W(x)$. \n",
    "\n",
    "Lets use the notation \n",
    " * $\\partial^{l}_{ij} = \\frac{\\partial}{\\partial W^{l}_{ij}}$ which is the partial derivative with respect to network weight $W^{(l)}_{ij}$ \n",
    " * recall that $W^{(l)}_{ij}$ is the network weight connecting layer node $j$ from layer $l$ to node $i$ of layer $l + 1$\n",
    " * remember that $i = 0, 1 \\dots m_{l + 1}$ and $j = 0, 1 \\dots m_l$, where we absorbed bias terms into the $0^{th}$ index. \n",
    "\n",
    "\n",
    "\n",
    "So here goes the chain rule ðŸ˜­ and lets to the weights for the final layer first... \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\partial^{L}_{ij}L_W(x) \n",
    "& = \\frac{1}{4} \\sum_{k = 1}^n \\partial^{L}_{ij}(y_{k}(x) - f_W(x)_{k})^4 \\\\\n",
    "& = \\sum_{k = 1}^n (y_{k}(x) - f_W(x)_{k})^3 \\partial^{L}_{ij}f_W(x)_{k} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "Fortunately, the variable $W^L_{ij}$ only appears in terms where $k = i$, and so all other terms with $k \\neq i$ have zero $\\partial^L_{ij}$-derivative, and we have \n",
    "$$\n",
    "\\partial^{L}_{ij} L_W(x) = (y_i(x) - f_W(x)_i)^3 \\partial^{L}_{ij}f_W(x)_{i} \\\\\n",
    "$$\n",
    "As expected, we need to know the gradient with respect to $i^{th}$ output of the network itself $\\partial^{L}_{ij}f_W(x)_i$. \n",
    "\n",
    "\n",
    "And that's another chain rule ðŸ˜­... \n",
    "$$\n",
    "\\begin{align*}\n",
    "& \\partial^{L}_{ij}f_W(x)_i \\\\\n",
    "&= \\partial^{L}_{ij} \\sigma\\left(\\sum_{j' = 0}^{m_L} W^{(L)}_{ij'}H^{L}_{j'} \\right) \\tag*{notice index starts from 0}\\\\\n",
    "&= \\sigma'\\left(\\sum_{j' = 0}^{m_L} W^{(L)}_{ij'}H^{L}_{j'}\\right)\\, \n",
    "   \\partial^{L}_{ij}\\sum_{j = 0}^{m_L} W^{(L)}_{ij'}H^{L}_{j'} \\tag*{chain rule} \\\\\n",
    "&= \\sigma'\\left(\\sum_{j' = 0}^{m_L} W^{(L)}_{ij'}H^{L}_{j'}\\right)\\, \n",
    "   \\partial^{L}_{ij} W^{(L)}_{ij}H^{L}_{j} \\tag*{non-$ij$ terms vanishes} \\\\\n",
    "&= \\sigma'\\left(\\sum_{j' = 0}^M W^{(2)}_{ij'}H_{j'}\\right)\\, H^L_{j}  \\tag*{$\\partial^{L}_{ij}W^L_{ij} =1$} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So far we have not restrict ourselve to a particular activation function. As long as we know the derivative $\\sigma'$ we are good. For sigmoid activation, we have (exercise) $\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$. So collecting all these together we have \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\partial^L_{ij} L_W(x) \n",
    "&= (y_i(x) - f_W(x)_i)^3 \\partial^L_{ij}f_W(x)_{i} \\\\\n",
    "&= (y_i(x) - f_W(x)_i)^3 \\sigma\\left(\\sum_{j' = 0}^M W^{(2)}_{ij'}H_{j'}\\right)\\left(1 - \\sigma\\left(\\sum_{j' = 0}^M W^{(2)}_{ij'}H_{j'}\\right) \\right)\\, H^L_{j} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb1f4e2",
   "metadata": {},
   "source": [
    "## Derivatives of lower layers weights and how backpropagation helps\n",
    "TODO...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91845627",
   "metadata": {},
   "source": [
    "---\n",
    "# Python Implementation\n",
    "---\n",
    "TODO ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9f0a45ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 - np.exp(-x))\n",
    "\n",
    "def sigmoid_diff(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def ext(x):\n",
    "    # extend the vector x by prepending a 1.\n",
    "    if len(x.shape) == 2: \n",
    "        n = x.shape[0]\n",
    "        return np.concatenate([np.ones((n, 1)), x], axis=1)\n",
    "    elif len(x.shape) == 1:\n",
    "        return np.concatenate([[1], x])\n",
    "\n",
    "class MultiLayerPerceptron:\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        hidden_layer_sizes,\n",
    "        input_dim=1,\n",
    "        output_dim=1,\n",
    "        activation=sigmoid,\n",
    "        activation_diff=sigmoid_diff, \n",
    "        learning_rate=0.001,\n",
    "        max_epoch=500, \n",
    "        batch_size=100\n",
    "    ):\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim        \n",
    "        self.activation = activation\n",
    "        self.activation_diff = activation_diff\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_epoch = max_epoch\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.weights = [np.random.rand(input_dim + 1, hidden_layer_sizes[0])]\n",
    "        last_layer_size = hidden_layer_sizes[0]\n",
    "        for size in hidden_layer_sizes[1:]:\n",
    "            shape = (last_layer_size + 1, size) # +1 to add bias node\n",
    "            self.weights.append(np.random.rand(*shape))\n",
    "            last_layer_size = size\n",
    "        self.weights.append(np.random.rand(last_layer_size + 1, output_dim))\n",
    "        \n",
    "        self.hidden_layer_values = []\n",
    "    \n",
    "    def forward_pass(self, x):\n",
    "        output = x\n",
    "        for weight in self.weights:\n",
    "            output = sigmoid(ext(output) @ weight) # we use row vector multiplication on the left instead.\n",
    "        return output\n",
    "    \n",
    "    def loss(self, x, y):\n",
    "        pred = self.forward_pass(x)\n",
    "        return np.sum((y - pred)**4)/4\n",
    "    \n",
    "    def backprop(self, x):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def grad(self, x, y):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def batch_grad(self, X, Y):\n",
    "        return np.mean(self.grad(x, y) for x, y in zip(X, Y))\n",
    "    \n",
    "    def step(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return self.forward_pass(X)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "860cc28e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.31658437],\n",
       "        [1.31658346]]),\n",
       " 0.1140939391636162)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[3, 2, 5], [1, 1, 1]])\n",
    "y = np.array([1, 2])\n",
    "clf = MultiLayerPerceptron([10, 5, 5, 5, 2], input_dim=x.shape[1])\n",
    "\n",
    "clf.forward_pass(x), clf.loss(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f314294",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
